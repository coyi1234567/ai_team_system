#!/usr/bin/env python3
"""
LoRAæ¨¡å‹åˆå¹¶è„šæœ¬
ç”¨äºå°†è®­ç»ƒå¥½çš„LoRA adapteråˆå¹¶åˆ°åŸºç¡€æ¨¡å‹ä¸­
"""

import os
import torch
from transformers import AutoTokenizer, AutoModel
from peft import PeftModel
from glm_config import ProjectConfig
import argparse

pc = ProjectConfig()

def merge_lora_model(lora_path, output_path, base_model_path=None):
    """
    åˆå¹¶LoRAæ¨¡å‹åˆ°åŸºç¡€æ¨¡å‹
    
    Args:
        lora_path: LoRA adapterè·¯å¾„
        output_path: åˆå¹¶åçš„æ¨¡å‹ä¿å­˜è·¯å¾„
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„
    """
    if base_model_path is None:
        base_model_path = pc.pre_model
    
    print(f"ğŸ”§ å¼€å§‹åˆå¹¶LoRAæ¨¡å‹...")
    print(f"   åŸºç¡€æ¨¡å‹: {base_model_path}")
    print(f"   LoRAè·¯å¾„: {lora_path}")
    print(f"   è¾“å‡ºè·¯å¾„: {output_path}")
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    print("ğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹...")
    base_model = AutoModel.from_pretrained(
        base_model_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    
    # åŠ è½½tokenizer
    print("ğŸ“¥ åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
    
    # åŠ è½½LoRA adapter
    print("ğŸ“¥ åŠ è½½LoRA adapter...")
    peft_model = PeftModel.from_pretrained(
        model=base_model,
        model_id=lora_path
    )
    
    # åˆå¹¶æ¨¡å‹
    print("ğŸ”— åˆå¹¶LoRAåˆ°åŸºç¡€æ¨¡å‹...")
    merged_model = peft_model.merge_and_unload()
    
    # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
    print(f"ğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ¨¡å‹åˆ° {output_path}...")
    os.makedirs(output_path, exist_ok=True)
    merged_model.save_pretrained(output_path)
    tokenizer.save_pretrained(output_path)
    
    print("âœ… LoRAæ¨¡å‹åˆå¹¶å®Œæˆ!")
    print(f"   åˆå¹¶åçš„æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")

def main():
    parser = argparse.ArgumentParser(description="åˆå¹¶LoRAæ¨¡å‹")
    parser.add_argument("--lora_path", type=str, required=True, help="LoRA adapterè·¯å¾„")
    parser.add_argument("--output_path", type=str, required=True, help="åˆå¹¶åçš„æ¨¡å‹ä¿å­˜è·¯å¾„")
    parser.add_argument("--base_model_path", type=str, default=None, help="åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥LoRAè·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.lora_path):
        print(f"âŒ é”™è¯¯: LoRAè·¯å¾„ä¸å­˜åœ¨: {args.lora_path}")
        return
    
    try:
        merge_lora_model(args.lora_path, args.output_path, args.base_model_path)
    except Exception as e:
        print(f"âŒ åˆå¹¶è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 