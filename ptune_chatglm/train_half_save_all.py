# -*- coding:utf-8 -*-
import os  # æ“ä½œç³»ç»Ÿæ¥å£ï¼Œç”¨äºå¤„ç†æ–‡ä»¶è·¯å¾„ç­‰
import time  # æ—¶é—´ç®¡ç†ï¼Œç”¨äºè®¡ç®—è®­ç»ƒè€—æ—¶å’Œ ETA
import copy  # å¯¹æ¨¡å‹è¿›è¡Œæ·±æ‹·è´ï¼Œé¿å…ä¿®æ”¹åŸæ¨¡å‹
import argparse  # è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ˆå¯é€‰æ‰©å±•ï¼‰
from functools import partial  # ç”¨äºå°†å‡½æ•°éƒ¨åˆ†å‚æ•°å›ºå®š
import peft  # PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰åº“ï¼Œç”¨äº LoRA
import torch
import torch.nn as nn
from utils.common_utils import CastOutputToFloat

# autocast æ˜¯ PyTorch æ··åˆç²¾åº¦è®­ç»ƒçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨
# åœ¨ GPU ä¸Šå¼€å¯æ··åˆç²¾åº¦å¯åŠ é€Ÿè®­ç»ƒå¹¶èŠ‚çœæ˜¾å­˜ï¼ŒCPU ä¸Šæ— æ•ˆæœ
from torch.cuda.amp import autocast as autocast

# Transformers ä¸€ä½“åŒ–æ¥å£
from transformers import (
    AutoTokenizer,       # è‡ªåŠ¨åŠ è½½ tokenizer
    AutoConfig,          # è‡ªåŠ¨åŠ è½½æ¨¡å‹é…ç½®
    AutoModel,           # è‡ªåŠ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    get_scheduler        # å­¦ä¹ ç‡è°ƒåº¦å™¨
)

# å¼•å…¥è‡ªå®šä¹‰å·¥å…·å‡½æ•°ï¼šæ—¶é—´æ ¼å¼è½¬æ¢ã€ä¿å­˜æ¨¡å‹ç­‰
from utils.common_utils import second2time, save_model
# å¼•å…¥æ•°æ®åŠ è½½å‡½æ•°
from data_handle.data_loader import get_data
# é¡¹ç›®é…ç½®ç±»ï¼Œé›†ä¸­ç®¡ç†è·¯å¾„åŠè¶…å‚æ•°
from glm_config import ProjectConfig

import os
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'

# å®ä¾‹åŒ–é…ç½®ï¼Œè¯»å–é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„ã€LoRA/P-tuning é€‰é¡¹ã€è¶…å‚ç­‰
pc = ProjectConfig()

def model2train():
    """
    æ„å»ºæ¨¡å‹ã€ä¼˜åŒ–å™¨åŠè°ƒåº¦å™¨ï¼Œæ‰§è¡Œè®­ç»ƒå¾ªç¯ï¼Œå¹¶åœ¨æŒ‡å®šé¢‘ç‡ä¿å­˜æ¨¡å‹å’Œè¯„ä¼°ã€‚
    """
    # 1. åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        pc.pre_model,
        trust_remote_code=True  # å…è®¸åŠ è½½è‡ªå®šä¹‰å®ç°
    )

    # 2. åŠ è½½æ¨¡å‹é…ç½®
    config = AutoConfig.from_pretrained(
        pc.pre_model,
        trust_remote_code=True
    )

    # 3. å¦‚æœä½¿ç”¨ P-tuningï¼Œåˆ™åœ¨ config ä¸­æ³¨å…¥å‰ç¼€é•¿åº¦ä¸æŠ•å½±æ–¹å¼
    if pc.use_ptuning:
        config.pre_seq_len = pc.pre_seq_len
        config.prefix_projection = pc.prefix_projection

    # 4. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    model = AutoModel.from_pretrained(
        pc.pre_model,
        config=config,
        trust_remote_code=True
    )

    # 5. è½¬ä¸º float32 ç²¾åº¦ï¼ˆç¡®ä¿å…¼å®¹ LoRA/PEFTï¼‰ .half()
    # model = model.float()
    model = model.half()
    # >>> torch.int8   (GPTQ ç”¨ int4/int8 æ‰“åŒ…åœ¨ char = int8)
    # 6. å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœæ˜¾å­˜
    model.gradient_checkpointing_enable()
    # 7. å…è®¸è¾“å…¥ embedding è®¡ç®—æ¢¯åº¦ï¼Œç”¨äº P-tuning
    model.enable_input_require_grads()
    # 8. ç¦ç”¨è‡ªå¸¦ç¼“å­˜ï¼Œé™ä½æ˜¾å­˜ä½¿ç”¨
    model.config.use_cache = False

    # 9. å¦‚æœä½¿ç”¨ P-tuningï¼Œéœ€è¦å°†å‰ç¼€ç¼–ç å™¨ä¹Ÿè½¬ä¸º float
    if pc.use_ptuning:
        model.transformer.prefix_encoder.float()

    # 10. å¦‚æœä½¿ç”¨ LoRA å¾®è°ƒï¼šå°è£… lm_headï¼Œä½¿å…¶è¾“å‡ºä¸º float32ï¼›æ„å»º LoRA é…ç½®å¹¶åº”ç”¨ PEFT
    if pc.use_lora:
        # a. lm_head è¾“å‡ºè½¬ float

        # model.lm_head = CastOutputToFloat(model.lm_head)
        # ChatGLM3-6B æ­£ç¡®å†™æ³•
        model.transformer.output_layer = CastOutputToFloat(model.transformer.output_layer)

        # b. æ„é€  LoRA é…ç½®
        peft_config = peft.LoraConfig(
            task_type=peft.TaskType.CAUSAL_LM,  # å› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡
            inference_mode=False,               # è®­ç»ƒé˜¶æ®µå…³é—­æ¨ç†ä¼˜åŒ–
            r=pc.lora_rank,                     # LoRA ä½ç§©çŸ©é˜µç§©
            lora_alpha=2*pc.lora_rank,                      # LoRA ç¼©æ”¾ç³»æ•°
            lora_dropout=0.1,                   # LoRA dropout æ¯”ä¾‹
        )
        # c. è·å– LoRA å¾®è°ƒåçš„æ¨¡å‹
        model = peft.get_peft_model(model, peft_config)

    # 11. å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU/CPUï¼‰
    # 11a. å¤šå¡å¹¶è¡Œ
    # model = nn.DataParallel(model)
    if torch.cuda.device_count() > 1:
        print("Let's use", torch.cuda.device_count(), "GPUs!")
        # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs
        model = nn.DataParallel(model)

    model.to(pc.device)

    # print('22222')
    # print(model.transformer.layers[0].attention.query_key_value.weight.dtype)
    # >>> torch.int8   (GPTQ ç”¨ int4/int8 æ‰“åŒ…åœ¨ char = int8)
    print(f'model--ã€‹{model}')
    # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
    #print('æ¨¡å‹è®­ç»ƒå‚æ•°ï¼š', model.print_trainable_parameters())
    # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯ï¼ˆå…¼å®¹ DataParallelï¼‰
    real_model = model.module if isinstance(model, nn.DataParallel) else model
    print('æ¨¡å‹è®­ç»ƒå‚æ•°ï¼š', real_model.print_trainable_parameters())

    # â€”â€” æ„é€ ä¼˜åŒ–å™¨ & å­¦ä¹ ç‡è°ƒåº¦å™¨ â€”â€” #
    # 12. æŒ‰æ˜¯å¦è¡°å‡å‚æ•°åˆ†ç»„ï¼šbias ä¸ LayerNorm.weight ä¸è¿›è¡Œ weight decay
    no_decay = ["bias", "LayerNorm.weight"]
    optimizer_grouped_parameters = [
        {
            "params": [
                p for n, p in model.named_parameters()
                if not any(nd in n for nd in no_decay)
            ],
            "weight_decay": pc.weight_decay,
        },
        {
            "params": [
                p for n, p in model.named_parameters()
                if any(nd in n for nd in no_decay)
            ],
            "weight_decay": 0.0,
        },
    ]
    optimizer = torch.optim.AdamW(
        optimizer_grouped_parameters,
        lr=pc.learning_rate
    )

    # 13. åŠ è½½è®­ç»ƒ/éªŒè¯æ•°æ®
    train_dataloader, dev_dataloader = get_data()
    # 14. è®¡ç®—æ€»è®­ç»ƒæ­¥æ•°ä¸ warmup æ­¥æ•°
    num_update_steps_per_epoch = len(train_dataloader)
    max_train_steps = pc.epochs * num_update_steps_per_epoch
    warm_steps = int(pc.warmup_ratio * max_train_steps)
    # 15. æ„é€ çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = get_scheduler(
        name='linear',
        optimizer=optimizer,
        num_warmup_steps=warm_steps,
        num_training_steps=max_train_steps,
    )

    # â€”â€” è®­ç»ƒå¾ªç¯ â€”â€” #
    loss_list = []
    tic_train = time.time()
    global_step = 0
    best_eval_loss = float('inf')

    for epoch in range(1, pc.epochs + 1):
        for batch in train_dataloader:
            # 16. å‰å‘ + loss è®¡ç®—
            if pc.use_lora:
                # LoRA æ—¶å¯ç”¨æ··åˆç²¾åº¦
                with torch.amp.autocast('cuda'):
                    outputs = model(
                        input_ids=batch['input_ids'].to(device=pc.device, dtype=torch.long),
                        labels=batch['labels'].to(device=pc.device, dtype=torch.long)
                    )
                    loss = outputs.loss
            else:
                outputs = model(
                    input_ids=batch['input_ids'].to(device=pc.device, dtype=torch.long),
                    labels=batch['labels'].to(device=pc.device, dtype=torch.long)
                )
                loss = outputs.loss

            # 17. åå‘ + å‚æ•°æ›´æ–° + å­¦ä¹ ç‡æ›´æ–°
            # å°†å¤š GPU ä¸Šçš„æŸå¤±èšåˆä¸ºå•ä¸ªæ ‡é‡
            loss = loss.mean()
            # æˆ–è€…ï¼šloss = loss.sum()
            optimizer.zero_grad(set_to_none=True)
            loss.backward()
            optimizer.step()
            lr_scheduler.step()

            # 18. è®°å½• loss å¹¶æ¨è¿› global_step
            loss_list.append(loss.item())
            global_step += 1

            # 19. æ—¥å¿—è¾“å‡ºï¼šæ¯ logging_steps æ­¥æ‰“å°ä¸€æ¬¡å¹³å‡ lossã€è®­ç»ƒé€Ÿåº¦ä¸ ETA
            if global_step % pc.logging_steps == 0:
                time_diff = time.time() - tic_train
                loss_avg = sum(loss_list) / len(loss_list)
                eta = second2time(
                    int((max_train_steps - global_step)
                        / (pc.logging_steps / time_diff))
                )
                print(
                    f"global step {global_step} "
                    f"({global_step/max_train_steps*100:02.2f}%), "
                    f"epoch {epoch}, loss {loss_avg:.5f}, "
                    f"speed {pc.logging_steps/time_diff:.2f} step/s, "
                    f"ETA {eta}"
                )
                tic_train = time.time()
                loss_list.clear()

            # 20. å®šæœŸä¿å­˜æ¨¡å‹ & åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
            if global_step % pc.save_freq == 0:
                cur_eval_loss = evaluate_model(model, dev_dataloader)
                print(f"Evaluation Loss: {cur_eval_loss:.5f}")

                # åªæœ‰å½“éªŒè¯ loss æ›´ä½æ—¶æ‰ä¿å­˜
                if cur_eval_loss < best_eval_loss:
                    print(f"â†» Eval loss improved: {best_eval_loss:.5f} â†’ {cur_eval_loss:.5f}")
                    best_eval_loss = cur_eval_loss

                    # åŸºç¡€è·¯å¾„
                    base_path = os.path.join(pc.save_dir, "best")
                    os.makedirs(base_path, exist_ok=True)

                    # å–å‡ºæœ€åŸå§‹çš„ model.moduleï¼ˆè§£é™¤ DataParallel åŒ…è£…ï¼‰
                    real_model = model.module if isinstance(model, nn.DataParallel) else model

                    # â€”â€” 1. ä¿å­˜ Prefix-Tuning å‰ç¼€ â€”â€” #
                    if pc.use_ptuning:
                        ptune_dir = os.path.join(base_path, "ptuning")
                        os.makedirs(ptune_dir, exist_ok=True)
                        # å‡è®¾å‰ç¼€ç¼–ç å™¨åœ¨ real_model.transformer.prefix_encoder
                        torch.save(
                            real_model.transformer.prefix_encoder.state_dict(),
                            os.path.join(ptune_dir, "prefix_encoder.bin")
                        )
                        print(f"âœ” Saved Prefix-Tuning to {ptune_dir}")

                    # â€”â€” 2. ä¿å­˜ LoRA Adapter â€”â€” #
                    if pc.use_lora:
                        lora_dir = os.path.join(base_path, "lora")
                        os.makedirs(lora_dir, exist_ok=True)
                        # PEFT è‡ªå¸¦æ¥å£ï¼Œåªä¿å­˜ adapter éƒ¨åˆ†
                        real_model.save_pretrained(lora_dir)
                        tokenizer.save_pretrained(lora_dir)
                        print(f"âœ” Saved LoRA adapter to {lora_dir}")

                    # â€”â€” 3. åˆå¹¶ LoRA å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹ â€”â€” #
                    merged_dir = os.path.join(base_path, "merged")
                    os.makedirs(merged_dir, exist_ok=True)

                    from peft import PeftModel

                    # åŠ è½½ LoRA
                    # ä»åŸºæ¨¡å‹ ID/è·¯å¾„é‡æ–°åŠ è½½çº¯ Base Model
                    from transformers import AutoModelForCausalLM
                    if pc.use_lora:
                        # é‡æ–°åŠ è½½çº¯ base model
                        base_model = AutoModelForCausalLM.from_pretrained(
                            pc.pre_model,
                            trust_remote_code=True,
                            config=real_model.config
                        )
                        # åŠ è½½ adapter å¹¶ merge# æ­£ç¡®åœ°æŠŠ adapter èå…¥ base_model
                        peft_model = PeftModel.from_pretrained(base_model, lora_dir)
                        merged_model = peft_model.merge_and_unload()
                    else:
                        merged_model = real_model

                    # ä¿å­˜åˆå¹¶ä¹‹åçš„å®Œæ•´æ¨¡å‹
                    merged_model.save_pretrained(merged_dir)
                    tokenizer.save_pretrained(merged_dir)
                    print(f"âœ” Saved merged model to {merged_dir}")

                    # æ‰“å°ä¸€ä¸‹æœ€ç»ˆä¿¡æ¯
                    print(f"ğŸ‰ All parts saved under {base_path} (eval loss={best_eval_loss:.4f})")
                    # é‡ç½®è®¡æ—¶
                    tic_train = time.time()
            torch.cuda.empty_cache()



def evaluate_model(model, dev_dataloader):
    """
    åœ¨éªŒè¯é›†ä¸Šè®¡ç®—å¹³å‡ lossï¼Œç”¨äºæ¨¡å‹è¯„ä¼°ä¸æ—©åœåˆ¤æ–­ã€‚

    Args:
        model: è®­ç»ƒä¸­çš„æ¨¡å‹å®ä¾‹
        dev_dataloader: éªŒè¯é›† DataLoader

    Returns:
        float: éªŒè¯é›†ä¸Šçš„å¹³å‡ loss
    """
    model.eval()  # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼ˆå…³é—­ dropout ç­‰ï¼‰
    loss_list = []

    # ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ŒåŠ é€Ÿå¹¶èŠ‚çœæ˜¾å­˜
    with torch.no_grad():
        for batch in dev_dataloader:
            if pc.use_lora:
                with autocast():
                    outputs = model(
                        input_ids=batch['input_ids'].to(device=pc.device, dtype=torch.long),
                        labels=batch['labels'].to(device=pc.device, dtype=torch.long)
                    )
                    loss = outputs.loss
            else:
                outputs = model(
                    input_ids=batch['input_ids'].to(device=pc.device, dtype=torch.long),
                    labels=batch['labels'].to(device=pc.device, dtype=torch.long)
                )
                loss = outputs.loss

            loss_list.append(loss.item())

    model.train()  # æ¢å¤åˆ°è®­ç»ƒæ¨¡å¼
    return sum(loss_list) / len(loss_list)


if __name__ == '__main__':
    # è„šæœ¬å…¥å£ï¼šç›´æ¥è°ƒç”¨è®­ç»ƒå‡½æ•°
    model2train()