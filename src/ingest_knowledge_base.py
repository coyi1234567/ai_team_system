#!/usr/bin/env python3
"""
æ‰¹é‡å¯¼å…¥AIå›¢é˜ŸçŸ¥è¯†åº“ï¼ˆHybrid RAGï¼‰
- éå† knowledge_base/ ä¸‹æ‰€æœ‰è§’è‰²ç›®å½•
- åˆ†å—ã€åµŒå…¥ã€å†™å…¥Chromaå‘é‡åº“å’Œæœ¬åœ°BM25ç´¢å¼•
- æ‰€æœ‰æ•°æ®ä¿å­˜åœ¨ vector_db/ ç›®å½•
"""
import os
from pathlib import Path
from typing import List, Dict, Mapping
from tqdm import tqdm
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle

# é…ç½®è·¯å¾„
BASE_DIR = Path(__file__).parent.parent
KNOWLEDGE_BASE_DIR = BASE_DIR / "knowledge_base"
VECTOR_DB_DIR = BASE_DIR / "vector_db"
VECTOR_DB_DIR.mkdir(exist_ok=True)

# å¥å‘é‡æ¨¡å‹
EMBED_MODEL = "BAAI/bge-small-zh-v1.5"
embedder = SentenceTransformer(EMBED_MODEL)

# åˆå§‹åŒ–Chroma
chroma_client = chromadb.Client(Settings(
    persist_directory=str(VECTOR_DB_DIR)
))

# è§’è‰²åˆ—è¡¨ = knowledge_base/ ä¸‹æ‰€æœ‰å­ç›®å½•
roles = [d.name for d in KNOWLEDGE_BASE_DIR.iterdir() if d.is_dir()]

for role in roles:
    print(f"\n[è§’è‰²] {role}")
    role_dir = KNOWLEDGE_BASE_DIR / role
    # è¯»å–æ‰€æœ‰txtæ–‡æ¡£
    docs = []
    doc_paths = list(role_dir.glob("*.txt"))
    for txt_file in doc_paths:
        with open(txt_file, "r", encoding="utf-8") as f:
            content = f.read()
            docs.append({
                "file": txt_file.name,
                "content": content
            })
    if not docs:
        print(f"  âš ï¸ æ— æ–‡æ¡£ï¼Œè·³è¿‡ {role}")
        continue
    # åˆ†å—ï¼ˆç®€å•æŒ‰æ®µè½åˆ†ï¼‰
    all_chunks = []
    for doc in docs:
        for para in doc["content"].split("\n\n"):
            chunk = para.strip()
            if chunk:
                all_chunks.append({
                    "text": chunk,
                    "file": doc["file"]
                })
    print(f"  å…± {len(all_chunks)} ä¸ªçŸ¥è¯†å—")
    # ç”Ÿæˆå‘é‡
    texts = [c["text"] for c in all_chunks]
    embeddings = embedder.encode(texts, show_progress_bar=True, batch_size=32)
    # å†™å…¥Chromaï¼ˆæ¯ä¸ªè§’è‰²ä¸€ä¸ªcollectionï¼‰
    collection = chroma_client.get_or_create_collection(
        name=f"{role}_kb"
    )
    ids = [f"{role}_{i}" for i in range(len(all_chunks))]
    metadatas = [{"file": str(c["file"])} for c in all_chunks]
    collection.upsert(
        ids=ids,
        embeddings=embeddings.tolist(),
        documents=texts,
        metadatas=metadatas
    )
    print(f"  âœ… å·²å†™å…¥Chromaå‘é‡åº“: {role}_kb")
    # æ„å»ºBM25/TF-IDFå…³é”®è¯ç´¢å¼•
    vectorizer = TfidfVectorizer(token_pattern=r"(?u)\\b\\w+\\b", max_features=4096)
    tfidf = vectorizer.fit_transform(texts)
    bm25_data = {
        "vectorizer": vectorizer,
        "tfidf": tfidf,
        "texts": texts,
        "metadatas": metadatas
    }
    with open(VECTOR_DB_DIR / f"{role}_bm25.pkl", "wb") as f:
        pickle.dump(bm25_data, f)
    print(f"  âœ… å·²ä¿å­˜BM25å…³é”®è¯ç´¢å¼•: {role}_bm25.pkl")

print("\nğŸ‰ æ‰€æœ‰è§’è‰²çŸ¥è¯†åº“å·²æ‰¹é‡å¯¼å…¥å¹¶æ„å»ºç´¢å¼•ï¼") 