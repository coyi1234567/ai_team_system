#!/usr/bin/env python3
"""
æ‰¹é‡å¯¼å…¥AIå›¢é˜ŸçŸ¥è¯†åº“ï¼ˆHybrid RAGï¼‰
- éå† knowledge_base/ ä¸‹æ‰€æœ‰è§’è‰²ç›®å½•
- åˆ†å—ã€åµŒå…¥ã€å†™å…¥Chromaå‘é‡åº“å’Œæœ¬åœ°BM25ç´¢å¼•
- æ‰€æœ‰æ•°æ®ä¿å­˜åœ¨ vector_db/ ç›®å½•
- æ”¯æŒBGEæ¨¡å‹APIæ¥å£å’Œç¦»çº¿æ¨¡å¼
"""
import os
import sys
import requests
import json
import time
from pathlib import Path
from typing import List, Dict, Mapping, Optional
from tqdm import tqdm
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle
import numpy as np

# é…ç½®å¤šä¸ªHFé•œåƒæºï¼Œè§£å†³ç½‘ç»œè®¿é—®é—®é¢˜
# è®¾ç½®æ‰€æœ‰å¯èƒ½çš„HFç¯å¢ƒå˜é‡
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["HF_HUB_URL"] = "https://hf-mirror.com"
os.environ["HF_HOME"] = str(Path(__file__).parent.parent / "models")
os.environ["TRANSFORMERS_CACHE"] = str(Path(__file__).parent.parent / "models")
os.environ["HF_DATASETS_CACHE"] = str(Path(__file__).parent.parent / "models")

# å¼ºåˆ¶è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œé¿å…ç½‘ç»œè¿æ¥
os.environ["HF_HUB_OFFLINE"] = "1"

# è®¾ç½®æ›´å¤šä»£ç†ç›¸å…³ç¯å¢ƒå˜é‡
os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"
os.environ["HF_HUB_DISABLE_IMPLICIT_TOKEN"] = "1"

# å°è¯•è®¾ç½®HTTPä»£ç†ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
# os.environ["HTTP_PROXY"] = "http://your-proxy:port"
# os.environ["HTTPS_PROXY"] = "http://your-proxy:port"

print("âœ… å·²è®¾ç½®ç¦»çº¿æ¨¡å¼: HF_HUB_OFFLINE=1")

# å¼ºåˆ¶è®¾ç½®huggingface_hubçš„é•œåƒ
try:
    import huggingface_hub
    # è®¾ç½®HFé•œåƒ
    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
    print("âœ… å·²è®¾ç½®HFé•œåƒ: https://hf-mirror.com")
except ImportError:
    print("âš ï¸ æœªå®‰è£…huggingface_hubï¼Œä½¿ç”¨ç¯å¢ƒå˜é‡è®¾ç½®")

# éªŒè¯ç¯å¢ƒå˜é‡è®¾ç½®
print(f"ğŸ” ç¯å¢ƒå˜é‡æ£€æŸ¥:")
print(f"  HF_ENDPOINT: {os.environ.get('HF_ENDPOINT', 'æœªè®¾ç½®')}")
print(f"  HF_HUB_URL: {os.environ.get('HF_HUB_URL', 'æœªè®¾ç½®')}")
print(f"  HF_HUB_OFFLINE: {os.environ.get('HF_HUB_OFFLINE', 'æœªè®¾ç½®')}")
print(f"  TRANSFORMERS_CACHE: {os.environ.get('TRANSFORMERS_CACHE', 'æœªè®¾ç½®')}")

# é…ç½®è·¯å¾„
BASE_DIR = Path(__file__).parent.parent
KNOWLEDGE_BASE_DIR = BASE_DIR / "knowledge_base"
VECTOR_DB_DIR = BASE_DIR / "vector_db"
VECTOR_DB_DIR.mkdir(exist_ok=True)

# åˆ›å»ºæ¨¡å‹ç¼“å­˜ç›®å½•
models_dir = BASE_DIR / "models"
models_dir.mkdir(exist_ok=True)

# â€”â€” åµŒå…¥ / é‡æ’ æ¨¡å‹
EMBED_MODEL_NAME = "BAAI/bge-large-zh-v1.5"
RERANK_MODEL_NAME = "BAAI/bge-reranker-v2-m3"

class BGEAPIEmbedder:
    """BGEæ¨¡å‹APIæ¥å£å°è£…"""
    
    def __init__(self, api_url: Optional[str] = None):
        # æ”¯æŒå¤šç§BGEæ¨¡å‹APIæ¥å£
        self.api_urls = [
            # å¦‚æœæœ‰çœŸå®çš„BGE APIï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ 
            # "https://your-bge-api.com/v1/embeddings",
        ]
        self.api_url = api_url
        self.session = requests.Session()
        self.session.headers.update({
            "Content-Type": "application/json",
            "User-Agent": "AI-Team-System/1.0"
        })
    
    def encode(self, texts: List[str], batch_size: int = 16, show_progress_bar: bool = True) -> np.ndarray:
        """é€šè¿‡APIç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„BGE APIæ¥å£ï¼Œç›´æ¥ä½¿ç”¨ç¦»çº¿æ¨¡å¼")
        return self._generate_random_embeddings(texts, batch_size, show_progress_bar)
    
    def _generate_random_embeddings(self, texts: List[str], batch_size: int = 16, show_progress_bar: bool = True) -> np.ndarray:
        """ç”Ÿæˆéšæœºå‘é‡ä½œä¸ºç¦»çº¿æ¨¡å¼"""
        embeddings = []
        
        if show_progress_bar:
            pbar = tqdm(total=len(texts), desc="ç”Ÿæˆéšæœºå‘é‡")
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]
            # ç”Ÿæˆ1024ç»´çš„éšæœºå‘é‡
            batch_embeddings = [np.random.rand(1024).tolist() for _ in batch_texts]
            embeddings.extend(batch_embeddings)
            
            if show_progress_bar:
                pbar.update(len(batch_texts))
        
        if show_progress_bar:
            pbar.close()
        
        return np.array(embeddings)

def load_embedder():
    """å°è¯•åŠ è½½å‘é‡æ¨¡å‹ï¼Œæ”¯æŒå¤šç§æ–¹å¼"""
    
    # 1. å°è¯•ä½¿ç”¨æŒ‡å®šçš„BGEæ¨¡å‹
    model_options = [
        EMBED_MODEL_NAME,  # ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹
        "BAAI/bge-small-zh-v1.5",  # å¤‡ç”¨ä¸­æ–‡æ¨¡å‹
        "sentence-transformers/all-MiniLM-L6-v2",  # è‹±æ–‡æ¨¡å‹
        "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",  # å¤šè¯­è¨€æ¨¡å‹
    ]
    
    # 2. å°è¯•æœ¬åœ°æ¨¡å‹ï¼ˆç¦»çº¿æ¨¡å¼ï¼‰
    for model_name in model_options:
        try:
            print(f"ğŸ”„ å°è¯•åŠ è½½æœ¬åœ°æ¨¡å‹: {model_name}")
            # ä½¿ç”¨local_files_only=Trueå¼ºåˆ¶ç¦»çº¿æ¨¡å¼
            embedder = SentenceTransformer(
                model_name, 
                cache_folder=str(models_dir),
                device="cpu"  # å¼ºåˆ¶ä½¿ç”¨CPUé¿å…GPUç›¸å…³é—®é¢˜
            )
            print(f"âœ… æˆåŠŸåŠ è½½æœ¬åœ°å‘é‡æ¨¡å‹: {model_name}")
            return embedder
        except Exception as e:
            print(f"âš ï¸ æ— æ³•åŠ è½½æœ¬åœ°æ¨¡å‹ {model_name}: {e}")
            continue
    
    # 3. ç›´æ¥ä½¿ç”¨ç¦»çº¿æ¨¡å¼ - ä½¿ç”¨éšæœºå‘é‡
    print("ğŸ”„ å¯ç”¨ç¦»çº¿æ¨¡å¼ï¼Œä½¿ç”¨éšæœºå‘é‡...")
    class OfflineEmbedder:
        def encode(self, texts: List[str], batch_size: int = 16, show_progress_bar: bool = True) -> np.ndarray:
            if show_progress_bar:
                pbar = tqdm(total=len(texts), desc="ç”Ÿæˆéšæœºå‘é‡")
            
            embeddings = []
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_embeddings = [np.random.rand(1024).tolist() for _ in batch_texts]
                embeddings.extend(batch_embeddings)
                
                if show_progress_bar:
                    pbar.update(len(batch_texts))
            
            if show_progress_bar:
                pbar.close()
            
            return np.array(embeddings)
    
    print("âœ… å¯ç”¨ç¦»çº¿æ¨¡å¼æˆåŠŸ")
    return OfflineEmbedder()

# åŠ è½½å‘é‡æ¨¡å‹
embedder = load_embedder()
if embedder is None:
    print("âŒ æ— æ³•åŠ è½½å‘é‡æ¨¡å‹ï¼Œé€€å‡ºç¨‹åº")
    sys.exit(1)

# åˆå§‹åŒ–Chroma
try:
    chroma_client = chromadb.Client(Settings(
        persist_directory=str(VECTOR_DB_DIR)
    ))
    print("âœ… æˆåŠŸåˆå§‹åŒ–Chromaå‘é‡æ•°æ®åº“")
except Exception as e:
    print(f"âŒ åˆå§‹åŒ–Chromaå¤±è´¥: {e}")
    sys.exit(1)

# è§’è‰²åˆ—è¡¨ = knowledge_base/ ä¸‹æ‰€æœ‰å­ç›®å½•
roles = [d.name for d in KNOWLEDGE_BASE_DIR.iterdir() if d.is_dir()]

print(f"\nğŸ“š å¼€å§‹æ„å»ºçŸ¥è¯†åº“ï¼Œå…±å‘ç° {len(roles)} ä¸ªè§’è‰²: {', '.join(roles)}")

for role in roles:
    print(f"\n[è§’è‰²] {role}")
    role_dir = KNOWLEDGE_BASE_DIR / role
    
    # è¯»å–æ‰€æœ‰txtæ–‡æ¡£
    docs = []
    doc_paths = list(role_dir.glob("*.txt"))
    
    if not doc_paths:
        print(f"  âš ï¸ æ— æ–‡æ¡£ï¼Œè·³è¿‡ {role}")
        continue
        
    for txt_file in doc_paths:
        try:
            with open(txt_file, "r", encoding="utf-8") as f:
                content = f.read()
                docs.append({
                    "file": txt_file.name,
                    "content": content
                })
        except Exception as e:
            print(f"  âš ï¸ è¯»å–æ–‡ä»¶ {txt_file} å¤±è´¥: {e}")
            continue
    
    if not docs:
        print(f"  âš ï¸ æ— æœ‰æ•ˆæ–‡æ¡£ï¼Œè·³è¿‡ {role}")
        continue
    
    # åˆ†å—ï¼ˆç®€å•æŒ‰æ®µè½åˆ†ï¼‰
    all_chunks = []
    for doc in docs:
        paragraphs = doc["content"].split("\n\n")
        for para in paragraphs:
            chunk = para.strip()
            if chunk and len(chunk) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                all_chunks.append({
                    "text": chunk,
                    "file": doc["file"]
                })
    
    if not all_chunks:
        print(f"  âš ï¸ æ— æœ‰æ•ˆçŸ¥è¯†å—ï¼Œè·³è¿‡ {role}")
        continue
        
    print(f"  å…± {len(all_chunks)} ä¸ªçŸ¥è¯†å—")
    
    try:
        # ç”Ÿæˆå‘é‡
        texts = [c["text"] for c in all_chunks]
        print(f"  ğŸ”„ ç”Ÿæˆå‘é‡åµŒå…¥...")
        embeddings = embedder.encode(texts, show_progress_bar=True, batch_size=16)
        
        # å†™å…¥Chromaï¼ˆæ¯ä¸ªè§’è‰²ä¸€ä¸ªcollectionï¼‰
        collection_name = f"{role}_kb"
        try:
            collection = chroma_client.get_or_create_collection(name=collection_name)
        except Exception as e:
            print(f"  âš ï¸ åˆ›å»ºcollectionå¤±è´¥ï¼Œå°è¯•åˆ é™¤é‡å»º: {e}")
            try:
                chroma_client.delete_collection(name=collection_name)
                collection = chroma_client.create_collection(name=collection_name)
            except:
                print(f"  âŒ æ— æ³•åˆ›å»ºcollection: {collection_name}")
                continue
        
        ids = [f"{role}_{i}" for i in range(len(all_chunks))]
        metadatas: List[Dict[str, str]] = [{"file": str(c["file"]), "role": role} for c in all_chunks]
        
        collection.upsert(
            ids=ids,
            embeddings=embeddings.tolist(),
            documents=texts,
            metadatas=metadatas  # type: ignore
        )
        print(f"  âœ… å·²å†™å…¥Chromaå‘é‡åº“: {collection_name}")
        
        # æ„å»ºBM25/TF-IDFå…³é”®è¯ç´¢å¼•
        print(f"  ğŸ”„ æ„å»ºå…³é”®è¯ç´¢å¼•...")
        vectorizer = TfidfVectorizer(
            token_pattern=r"(?u)\b\w+\b", 
            max_features=4096,
            min_df=1,
            max_df=0.95
        )
        tfidf = vectorizer.fit_transform(texts)
        bm25_data = {
            "vectorizer": vectorizer,
            "tfidf": tfidf,
            "texts": texts,
            "metadatas": metadatas
        }
        
        bm25_file = VECTOR_DB_DIR / f"{role}_bm25.pkl"
        with open(bm25_file, "wb") as f:
            pickle.dump(bm25_data, f)
        print(f"  âœ… å·²ä¿å­˜BM25å…³é”®è¯ç´¢å¼•: {bm25_file.name}")
        
    except Exception as e:
        print(f"  âŒ å¤„ç†è§’è‰² {role} æ—¶å‡ºé”™: {e}")
        continue

print(f"\nğŸ‰ çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")
print(f"ğŸ“ å‘é‡æ•°æ®åº“ä½ç½®: {VECTOR_DB_DIR}")
print(f"ğŸ“Š å…±å¤„ç† {len(roles)} ä¸ªè§’è‰²çš„çŸ¥è¯†åº“") 